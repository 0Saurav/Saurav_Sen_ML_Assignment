{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edf0e3b5",
   "metadata": {},
   "source": [
    "\n",
    "Nepal District Vulnerability Analysis - Data Inspection\n",
    "\n",
    "Step 1: Inspect all raw CSV files to understand structure\n",
    "\n",
    "Author: Saurav Sen\n",
    "Date: 2025 December 04\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a030234c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DETAILED DATA INSPECTION - COMPREHENSIVE ANALYSIS\n",
      "================================================================================\n",
      "Found 16 CSV files\n",
      "\n",
      "Inspecting all files...\n",
      "Processing  1/16: below_secondary_education.csv\n",
      "Processing  2/16: children_living_arrangement.csv\n",
      "Processing  3/16: cooking_fuel.csv\n",
      "\n",
      "============================================================\n",
      "KEY FILE: cooking_fuel.csv\n",
      "============================================================\n",
      "Rows: 90, Columns: 9\n",
      "District column: 'AREA'\n",
      "Unique areas: 90\n",
      "Format: Wide\n",
      "\n",
      "Processing  4/16: drinking_watersource.csv\n",
      "\n",
      "============================================================\n",
      "KEY FILE: drinking_watersource.csv\n",
      "============================================================\n",
      "Rows: 90, Columns: 11\n",
      "District column: 'AREA'\n",
      "Unique areas: 90\n",
      "Format: Wide\n",
      "\n",
      "Processing  5/16: educational_attainment.csv\n",
      "Processing  6/16: educational_field_distribution.csv\n",
      "Processing  7/16: floor_type.csv\n",
      "Processing  8/16: foundation_type.csv\n",
      "Processing  9/16: household_amenities.csv\n",
      "\n",
      "============================================================\n",
      "KEY FILE: household_amenities.csv\n",
      "============================================================\n",
      "Rows: 90, Columns: 19\n",
      "District column: 'AREA_TYPE'\n",
      "Unique areas: 5\n",
      "Format: Long\n",
      "\n",
      "Processing 10/16: housing_ownership.csv\n",
      "Processing 11/16: lighting_source.csv\n",
      "Processing 12/16: months_worked.csv\n",
      "Processing 13/16: population_occupation.csv\n",
      "Processing 14/16: roof_type.csv\n",
      "Processing 15/16: toilet_facility.csv\n",
      "Processing 16/16: wall_materials.csv\n",
      "\n",
      "============================================================\n",
      "KEY FILE: wall_materials.csv\n",
      "============================================================\n",
      "Rows: 90, Columns: 10\n",
      "District column: 'AREA'\n",
      "Unique areas: 90\n",
      "Format: Wide\n",
      "\n",
      "\n",
      "✓ Detailed inspection saved to: C:\\Users\\saurav\\Downloads\\SEVI_Nepal_Project\\data\\processed\\detailed_inspection.txt\n",
      "✓ Summary table saved to: C:\\Users\\saurav\\Downloads\\SEVI_Nepal_Project\\data\\processed\\file_summary.csv\n",
      "\n",
      "================================================================================\n",
      "QUICK RECOMMENDATIONS:\n",
      "================================================================================\n",
      "\n",
      "wall_materials.csv analysis:\n",
      "  Columns: ['ID', 'AREA', 'MUD_BONDED_BRICKS_STONE', 'CEMENT_BONDED_BRICKS_STONE', 'WOOD_PLANKS', 'BAMBOO', 'UNBAKED_BRICKS', 'GALVANIZED_SHEET', 'PREFABRICATED_SHEET', 'OTHER']\n",
      "  No 'District' column found!\n",
      "  Check for alternative area column names\n",
      "\n",
      "================================================================================\n",
      "SHARE THIS INFORMATION:\n",
      "================================================================================\n",
      "\n",
      "Please share:\n",
      "1. The output file: data/processed/detailed_inspection.txt\n",
      "2. Specifically for wall_materials.csv:\n",
      "   - All column names\n",
      "   - Whether it's Long or Wide format\n",
      "   - What varies across rows (e.g., material types)\n",
      "\n",
      "This will determine if we need to pivot the data.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup paths - USE RAW STRING FOR WINDOWS\n",
    "RAW_PATH = Path(r'C:\\Users\\saurav\\Downloads\\SEVI_Nepal_Project\\data\\raw')\n",
    "PROCESSED_PATH = Path(r'C:\\Users\\saurav\\Downloads\\SEVI_Nepal_Project\\data\\processed')\n",
    "PROCESSED_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DETAILED DATA INSPECTION - COMPREHENSIVE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# List all CSV files\n",
    "csv_files = list(RAW_PATH.glob('*.csv'))\n",
    "print(f\"Found {len(csv_files)} CSV files\")\n",
    "\n",
    "# Function to inspect a file in detail - IMPROVED VERSION\n",
    "def inspect_file_details(file_path):\n",
    "    \"\"\"Detailed inspection of a CSV file\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, low_memory=False)  # Added low_memory for safety\n",
    "        filename = file_path.name\n",
    "        \n",
    "        # Prepare output\n",
    "        output_lines = []\n",
    "        output_lines.append(\"=\"*80)\n",
    "        output_lines.append(f\"FILE: {filename}\")\n",
    "        output_lines.append(\"=\"*80)\n",
    "        output_lines.append(f\"Shape: {df.shape} (rows × columns)\")\n",
    "        \n",
    "        # Memory usage\n",
    "        memory_mb = df.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "        output_lines.append(f\"Memory usage: {memory_mb:.2f} MB\")\n",
    "        \n",
    "        # Columns with more details\n",
    "        output_lines.append(f\"\\nCOLUMNS ({len(df.columns)} total):\")\n",
    "        for i, col in enumerate(df.columns, 1):\n",
    "            col_str = f\"'{col}'\"\n",
    "            dtype_str = f\"{df[col].dtype}\"\n",
    "            \n",
    "            # Count unique values for categorical columns\n",
    "            if df[col].dtype == 'object' or df[col].nunique() < 20:\n",
    "                unique_count = df[col].nunique()\n",
    "                dtype_str = f\"{dtype_str} ({unique_count} unique)\"\n",
    "            \n",
    "            output_lines.append(f\"{i:3}. {col_str:40} {dtype_str}\")\n",
    "        \n",
    "        # Find district/area column with more flexibility\n",
    "        area_cols = []\n",
    "        for col in df.columns:\n",
    "            col_lower = str(col).lower()\n",
    "            if any(keyword in col_lower for keyword in ['district', 'dist', 'mun', 'area', 'vdc']):\n",
    "                area_cols.append(col)\n",
    "        \n",
    "        if area_cols:\n",
    "            output_lines.append(f\"\\nAREA/DISTRICT COLUMNS ({len(area_cols)} found):\")\n",
    "            for col in area_cols:\n",
    "                unique_count = df[col].nunique()\n",
    "                sample_vals = list(df[col].dropna().unique()[:3])\n",
    "                output_lines.append(f\"  '{col}': {unique_count} unique values\")\n",
    "                output_lines.append(f\"    Sample: {sample_vals}\")\n",
    "                \n",
    "                # Check for 'Total' or aggregate rows\n",
    "                if df[col].dtype == 'object':\n",
    "                    total_rows = df[col].str.contains('total|Total|TOTAL|Nepal|Province', na=False).sum()\n",
    "                    if total_rows > 0:\n",
    "                        output_lines.append(f\"    Contains {total_rows} aggregate rows (Total/National/Province)\")\n",
    "        else:\n",
    "            output_lines.append(\"\\nWARNING: No district/area column found!\")\n",
    "        \n",
    "        # Find ID column\n",
    "        id_cols = [col for col in df.columns if 'id' in str(col).lower() or 'code' in str(col).lower()]\n",
    "        if id_cols:\n",
    "            output_lines.append(f\"\\nID COLUMNS ({len(id_cols)} found):\")\n",
    "            for col in id_cols:\n",
    "                dtype = df[col].dtype\n",
    "                unique_count = df[col].nunique()\n",
    "                output_lines.append(f\"  '{col}': {dtype}, {unique_count} unique values\")\n",
    "        \n",
    "        # Missing values analysis - IMPROVED\n",
    "        output_lines.append(\"\\nMISSING VALUES ANALYSIS:\")\n",
    "        missing_total = df.isnull().sum().sum()\n",
    "        missing_percentage = (missing_total / (df.shape[0] * df.shape[1])) * 100\n",
    "        \n",
    "        output_lines.append(f\"  Total missing cells: {missing_total:,}\")\n",
    "        output_lines.append(f\"  Percentage missing: {missing_percentage:.1f}%\")\n",
    "        \n",
    "        # Columns with missing values\n",
    "        missing_cols = df.isnull().sum()\n",
    "        missing_cols = missing_cols[missing_cols > 0]\n",
    "        \n",
    "        if len(missing_cols) > 0:\n",
    "            output_lines.append(f\"  Columns with missing values: {len(missing_cols)}\")\n",
    "            for col, count in missing_cols.nlargest(10).items():  # Top 10\n",
    "                pct = count / len(df) * 100\n",
    "                output_lines.append(f\"    '{col}': {count} ({pct:.1f}%)\")\n",
    "        \n",
    "        # Determine format (Long vs Wide)\n",
    "        if area_cols:\n",
    "            primary_area_col = area_cols[0]\n",
    "            value_counts = df[primary_area_col].value_counts()\n",
    "            \n",
    "            output_lines.append(\"\\nDATA STRUCTURE ANALYSIS:\")\n",
    "            if len(value_counts) == len(df):\n",
    "                output_lines.append(\"  FORMAT: Wide (1 row per district/area)\")\n",
    "                output_lines.append(\"  NOTE: May need to filter out aggregate rows (Total, Province)\")\n",
    "            else:\n",
    "                output_lines.append(\"  FORMAT: Long (Multiple rows per district/area)\")\n",
    "                output_lines.append(f\"  Average rows per district: {len(df)/len(value_counts):.1f}\")\n",
    "                output_lines.append(f\"  Min rows per district: {value_counts.min()}\")\n",
    "                output_lines.append(f\"  Max rows per district: {value_counts.max()}\")\n",
    "                \n",
    "                # Identify what varies across rows (e.g., age groups, materials)\n",
    "                # Look for categorical columns with few unique values\n",
    "                categorical_cols = []\n",
    "                for col in df.columns:\n",
    "                    if col not in area_cols + id_cols:\n",
    "                        if df[col].nunique() < 20:\n",
    "                            categorical_cols.append((col, df[col].nunique()))\n",
    "                \n",
    "                if categorical_cols:\n",
    "                    output_lines.append(\"  Likely breakdown categories:\")\n",
    "                    for col, count in categorical_cols[:5]:  # Top 5\n",
    "                        sample_vals = list(df[col].dropna().unique()[:3])\n",
    "                        output_lines.append(f\"    '{col}' ({count} categories): {sample_vals}\")\n",
    "        \n",
    "        # Data sample - smarter selection\n",
    "        output_lines.append(\"\\nDATA SAMPLE (First 3 rows):\")\n",
    "        output_lines.append(df.head(3).to_string())\n",
    "        \n",
    "        # Summary statistics for numeric columns\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        if len(numeric_cols) > 0:\n",
    "            output_lines.append(\"\\nNUMERIC COLUMNS SUMMARY (first 3):\")\n",
    "            for col in numeric_cols[:3]:\n",
    "                output_lines.append(f\"  '{col}':\")\n",
    "                output_lines.append(f\"    Min: {df[col].min():.2f}, Max: {df[col].max():.2f}\")\n",
    "                output_lines.append(f\"    Mean: {df[col].mean():.2f}, Std: {df[col].std():.2f}\")\n",
    "                if df[col].notna().sum() > 0:\n",
    "                    output_lines.append(f\"    Non-zero: {(df[col] != 0).sum()} rows\")\n",
    "        \n",
    "        return \"\\n\".join(output_lines), df, area_cols[0] if area_cols else None\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"ERROR reading {file_path.name}: {str(e)}\"\n",
    "        return error_msg, None, None\n",
    "\n",
    "# Also create a summary dataframe\n",
    "summary_data = []\n",
    "\n",
    "print(\"\\nInspecting all files...\")\n",
    "\n",
    "with open(PROCESSED_PATH / 'detailed_inspection.txt', 'w', encoding='utf-8') as f:\n",
    "    # Write header\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"DETAILED INSPECTION OF ALL CSV FILES\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    \n",
    "    # Process each file\n",
    "    for i, csv_file in enumerate(csv_files, 1):\n",
    "        print(f\"Processing {i:2}/{len(csv_files)}: {csv_file.name}\")\n",
    "        inspection_output, df, district_col = inspect_file_details(csv_file)\n",
    "        f.write(inspection_output + \"\\n\\n\")\n",
    "        \n",
    "        # Add to summary\n",
    "        if df is not None:\n",
    "            summary_data.append({\n",
    "                'Filename': csv_file.name,\n",
    "                'Rows': df.shape[0],\n",
    "                'Columns': df.shape[1],\n",
    "                'District_Column': district_col if district_col else 'None',\n",
    "                'Format': 'Wide' if district_col and df[district_col].nunique() == len(df) else 'Long',\n",
    "                'Missing_%': (df.isnull().sum().sum() / (df.shape[0] * df.shape[1]) * 100),\n",
    "                'Memory_MB': df.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "            })\n",
    "        \n",
    "        # Print key files to screen\n",
    "        key_files = ['wall_materials.csv', 'drinking_watersource.csv', 'cooking_fuel.csv',\n",
    "                    'toilet_facilities.csv', 'household_amenities.csv']\n",
    "        \n",
    "        if csv_file.name in key_files:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(f\"KEY FILE: {csv_file.name}\")\n",
    "            print(\"=\"*60)\n",
    "            if df is not None:\n",
    "                print(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
    "                print(f\"District column: '{district_col}'\" if district_col else \"No district column\")\n",
    "                if district_col:\n",
    "                    print(f\"Unique areas: {df[district_col].nunique()}\")\n",
    "                    print(f\"Format: {'Wide' if df[district_col].nunique() == len(df) else 'Long'}\")\n",
    "                print()\n",
    "\n",
    "# Create summary dataframe\n",
    "if summary_data:\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Save summary to CSV\n",
    "    summary_df.to_csv(PROCESSED_PATH / 'file_summary.csv', index=False)\n",
    "    \n",
    "    # Also add to text file\n",
    "    with open(PROCESSED_PATH / 'detailed_inspection.txt', 'a', encoding='utf-8') as f:\n",
    "        f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "        f.write(\"SUMMARY TABLE FOR ALL FILES\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        f.write(summary_df.to_string())\n",
    "        \n",
    "        # Recommendations\n",
    "        f.write(\"\\n\\n\" + \"=\"*80 + \"\\n\")\n",
    "        f.write(\"RECOMMENDATIONS\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        # Check for wall_materials format\n",
    "        wall_file = RAW_PATH / 'wall_materials.csv'\n",
    "        if wall_file.exists():\n",
    "            wall_df = pd.read_csv(wall_file)\n",
    "            if 'District' in wall_df.columns:\n",
    "                if wall_df['District'].nunique() == len(wall_df):\n",
    "                    f.write(\"1. wall_materials.csv is in WIDE format\\n\")\n",
    "                    f.write(\"   → Can merge directly by District\\n\")\n",
    "                else:\n",
    "                    f.write(\"1. wall_materials.csv is in LONG format\\n\")\n",
    "                    f.write(\"   → Need to pivot to wide format before merging\\n\")\n",
    "\n",
    "print(f\"\\n✓ Detailed inspection saved to: {PROCESSED_PATH / 'detailed_inspection.txt'}\")\n",
    "print(f\"✓ Summary table saved to: {PROCESSED_PATH / 'file_summary.csv'}\")\n",
    "\n",
    "# Quick recommendations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUICK RECOMMENDATIONS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check wall_materials specifically\n",
    "wall_path = RAW_PATH / 'wall_materials.csv'\n",
    "if wall_path.exists():\n",
    "    wall_df = pd.read_csv(wall_path)\n",
    "    print(f\"\\nwall_materials.csv analysis:\")\n",
    "    print(f\"  Columns: {list(wall_df.columns)}\")\n",
    "    \n",
    "    if 'District' in wall_df.columns:\n",
    "        unique_districts = wall_df['District'].nunique()\n",
    "        total_rows = len(wall_df)\n",
    "        \n",
    "        print(f\"  Unique districts: {unique_districts}\")\n",
    "        print(f\"  Total rows: {total_rows}\")\n",
    "        \n",
    "        if total_rows > unique_districts:\n",
    "            print(f\"  FORMAT: LONG (multiple rows per district)\")\n",
    "            print(f\"  Likely categories: {[col for col in wall_df.columns if col != 'District' and wall_df[col].nunique() < 20]}\")\n",
    "            print(\"\\n  ACTION NEEDED: Pivot to wide format\")\n",
    "            print(\"  Example: df.pivot(index='District', columns='Material_Type', values='Count')\")\n",
    "        else:\n",
    "            print(f\"  FORMAT: WIDE (1 row per district)\")\n",
    "            print(\"\\n  ACTION: Can merge directly by district name\")\n",
    "    else:\n",
    "        print(\"  No 'District' column found!\")\n",
    "        print(\"  Check for alternative area column names\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SHARE THIS INFORMATION:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nPlease share:\")\n",
    "print(\"1. The output file: data/processed/detailed_inspection.txt\")\n",
    "print(\"2. Specifically for wall_materials.csv:\")\n",
    "print(\"   - All column names\")\n",
    "print(\"   - Whether it's Long or Wide format\")\n",
    "print(\"   - What varies across rows (e.g., material types)\")\n",
    "print(\"\\nThis will determine if we need to pivot the data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
