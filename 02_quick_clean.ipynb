{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b438ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBUST DATA EXTRACTION - 77 DISTRICTS\n",
      "============================================================\n",
      "\n",
      "Checking available files...\n",
      "Found 16 CSV files\n",
      " 1. below_secondary_education.csv\n",
      " 2. children_living_arrangement.csv\n",
      " 3. cooking_fuel.csv\n",
      " 4. drinking_watersource.csv\n",
      " 5. educational_attainment.csv\n",
      " 6. educational_field_distribution.csv\n",
      " 7. floor_type.csv\n",
      " 8. foundation_type.csv\n",
      " 9. household_amenities.csv\n",
      "10. housing_ownership.csv\n",
      "11. lighting_source.csv\n",
      "12. months_worked.csv\n",
      "13. population_occupation.csv\n",
      "14. roof_type.csv\n",
      "15. toilet_facility.csv\n",
      "16. wall_materials.csv\n",
      "\n",
      "============================================================\n",
      "ANALYZING cooking_fuel.csv FOR DISTRICT IDS\n",
      "============================================================\n",
      "File shape: (90, 9)\n",
      "Columns: ['ID', 'AREA', 'WOOD_FIREWOOD', 'LPG_GAS', 'ELECTRICITY', 'COW_DUNG', 'BIOGAS', 'KEROSENE', 'OTHER']\n",
      "\n",
      "First 25 rows:\n",
      "ID   1: Nepal\n",
      "ID   2: Urban Municipalities\n",
      "ID   3: Rural Municipalities\n",
      "ID   4: Mountain\n",
      "ID   5: Hill\n",
      "ID   6: Tarai\n",
      "ID   7: Koshi\n",
      "ID   8: Madhesh\n",
      "ID   9: Bagmati\n",
      "ID  10: Gandaki\n",
      "ID  11: Lumbini\n",
      "ID  12: Karnali\n",
      "ID  13: Sudur Paschim\n",
      "ID  14: Taplejung\n",
      "ID  15: Sankhuwasabha\n",
      "ID  16: Solukhumbu\n",
      "ID  17: Okhaldhunga\n",
      "ID  18: Khotang\n",
      "ID  19: Bhojpur\n",
      "ID  20: Dhankuta\n",
      "ID  21: Terhathum\n",
      "ID  22: Panchthar\n",
      "ID  23: Ilam\n",
      "ID  24: Jhapa\n",
      "ID  25: Morang\n",
      "\n",
      "ID analysis:\n",
      "  Min ID: 1\n",
      "  Max ID: 90\n",
      "  Unique IDs: 90\n",
      "\n",
      "Identifying sections:\n",
      "  National aggregates (ID 1-6): 6 rows\n",
      "  Provinces (ID 7-13): 7 rows\n",
      "    Province names: ['Koshi', 'Madhesh', 'Bagmati', 'Gandaki', 'Lumbini', 'Karnali', 'Sudur Paschim']\n",
      "  Districts (ID 14+): 77 rows\n",
      "  ✓ Perfect! Found all 77 districts\n",
      "\n",
      "✓ District reference saved with 77 districts\n",
      "\n",
      "First 5 districts:\n",
      " ID DISTRICT_NAME\n",
      " 14     Taplejung\n",
      " 15 Sankhuwasabha\n",
      " 16    Solukhumbu\n",
      " 17   Okhaldhunga\n",
      " 18       Khotang\n",
      "\n",
      "Last 5 districts:\n",
      " ID DISTRICT_NAME\n",
      " 86    Dadeldhura\n",
      " 87          Doti\n",
      " 88        Achham\n",
      " 89       Kailali\n",
      " 90    Kanchanpur\n",
      "\n",
      "District ID range: 14 to 90\n",
      "\n",
      "============================================================\n",
      "CHECKING KEY FILES FOR DISTRICT DATA\n",
      "============================================================\n",
      "\n",
      "wall_materials.csv:\n",
      "  Shape: 90 rows × 10 cols\n",
      "  Has ID: Yes\n",
      "  District rows: 77\n",
      "\n",
      "drinking_watersource.csv:\n",
      "  Shape: 90 rows × 11 cols\n",
      "  Has ID: Yes\n",
      "  District rows: 77\n",
      "\n",
      "toilet_facility.csv:\n",
      "  Shape: 90 rows × 7 cols\n",
      "  Has ID: Yes\n",
      "  District rows: 77\n",
      "\n",
      "cooking_fuel.csv:\n",
      "  Shape: 90 rows × 9 cols\n",
      "  Has ID: Yes\n",
      "  District rows: 77\n",
      "\n",
      "household_amenities.csv:\n",
      "  Shape: 90 rows × 19 cols\n",
      "  Has ID: No\n",
      "\n",
      "main_source_of_lighting.csv: FILE NOT FOUND\n",
      "\n",
      "radio_television_etc.csv: FILE NOT FOUND\n",
      "\n",
      "============================================================\n",
      "CREATING MERGED DATASET\n",
      "============================================================\n",
      "Starting with 77 districts\n",
      "  ⚠ Duplicate columns: {'ID'}\n",
      "  ✓ wall_materials.csv: Added 8 columns\n",
      "  ⚠ Duplicate columns: {'ID'}\n",
      "  ✓ drinking_watersource.csv: Added 9 columns\n",
      "  ⚠ Duplicate columns: {'ID'}\n",
      "  ✓ toilet_facility.csv: Added 5 columns\n",
      "  ⚠ main_source_of_lighting.csv: File not found\n",
      "  ⚠ radio_television_etc.csv: File not found\n",
      "\n",
      "============================================================\n",
      "PROCESSING household_amenities.csv\n",
      "============================================================\n",
      "  Shape: (90, 19)\n",
      "  Columns: ['AREA_TYPE', 'AREA_NAME', 'TOTAL_HOUSEHOLDS', 'WITHOUT_ANY_AMENITIES', 'AT_LEAST_ONE_AMENITY', 'RADIO', 'TELEVISION', 'LANDLINE_TELEPHONE', 'MOBILE_PHONE_ORDINARY', 'MOBILE_PHONE_SMART', 'COMPUTER_LAPTOP', 'INTERNET', 'CAR_JEEP_VAN', 'MOTORCYCLE_SCOOTER', 'BICYCLE', 'ELECTRIC_FAN', 'REFRIGERATOR', 'WASHING_MACHINE', 'AIR_CONDITIONER']\n",
      "  District rows: 84\n",
      "  ✓ Added 17 columns from household_amenities\n",
      "\n",
      "============================================================\n",
      "FINAL DATASET\n",
      "============================================================\n",
      "\n",
      "Dataset shape: (77, 41)\n",
      "Districts: 77\n",
      "Total features: 41\n",
      "\n",
      "Columns with missing values: 17\n",
      "Top 5 columns with most missing:\n",
      "  HA_TOTAL_HOUSEHOLDS           :   5 (6.5%)\n",
      "  HA_WITHOUT_ANY_AMENITIES      :   5 (6.5%)\n",
      "  HA_AT_LEAST_ONE_AMENITY       :   5 (6.5%)\n",
      "  HA_RADIO                      :   5 (6.5%)\n",
      "  HA_TELEVISION                 :   5 (6.5%)\n",
      "\n",
      "✓ Merged dataset saved to: C:\\Users\\saurav\\Downloads\\SEVI_Nepal_Project\\data\\processed\\simple_merged_districts.csv\n",
      "✓ Sample saved to: C:\\Users\\saurav\\Downloads\\SEVI_Nepal_Project\\data\\processed\\district_sample.csv\n",
      "\n",
      "Dataset structure:\n",
      "----------------------------------------\n",
      "Column                                   Type       Non-Null  \n",
      "----------------------------------------\n",
      "ID                                       int64              77\n",
      "DISTRICT_NAME                            object             77\n",
      "WAL_MUD_BONDED_BRICKS_STONE              int64              77\n",
      "WAL_CEMENT_BONDED_BRICKS_STONE           int64              77\n",
      "WAL_WOOD_PLANKS                          int64              77\n",
      "WAL_BAMBOO                               int64              77\n",
      "WAL_UNBAKED_BRICKS                       int64              77\n",
      "WAL_GALVANIZED_SHEET                     int64              77\n",
      "WAL_PREFABRICATED_SHEET                  int64              77\n",
      "WAL_OTHER                                int64              77\n",
      "DRI_TAP_PIPED_WITHIN_PREMISES            int64              77\n",
      "DRI_TAP_PIPED_OUTSIDE_PREMISES           int64              77\n",
      "DRI_TUBEWELL_HANDPUMP                    int64              77\n",
      "DRI_COVERED_WELL_KUWA                    int64              77\n",
      "DRI_UNCOVERED_WELL_KUWA                  int64              77\n",
      "DRI_SPOUT_WATER                          int64              77\n",
      "DRI_RIVER_STREAM                         int64              77\n",
      "DRI_JAR_BOTTLE                           int64              77\n",
      "DRI_OTHER_SOURCES                        int64              77\n",
      "TOI_FLUSH_PUBLIC_SEWERAGE                int64              77\n",
      "TOI_FLUSH_SEPTIC_TANK                    int64              77\n",
      "TOI_PIT_TOILET                           int64              77\n",
      "TOI_PUBLIC_TOILET                        int64              77\n",
      "TOI_WITHOUT_TOILET_FACILITY              int64              77\n",
      "HA_TOTAL_HOUSEHOLDS                      float64            72\n",
      "HA_WITHOUT_ANY_AMENITIES                 float64            72\n",
      "HA_AT_LEAST_ONE_AMENITY                  float64            72\n",
      "HA_RADIO                                 float64            72\n",
      "HA_TELEVISION                            float64            72\n",
      "HA_LANDLINE_TELEPHONE                    float64            72\n",
      "HA_MOBILE_PHONE_ORDINARY                 float64            72\n",
      "HA_MOBILE_PHONE_SMART                    float64            72\n",
      "HA_COMPUTER_LAPTOP                       float64            72\n",
      "HA_INTERNET                              float64            72\n",
      "HA_CAR_JEEP_VAN                          float64            72\n",
      "HA_MOTORCYCLE_SCOOTER                    float64            72\n",
      "HA_BICYCLE                               float64            72\n",
      "HA_ELECTRIC_FAN                          float64            72\n",
      "HA_REFRIGERATOR                          float64            72\n",
      "HA_WASHING_MACHINE                       float64            72\n",
      "HA_AIR_CONDITIONER                       float64            72\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "1. Processed files: 6 (+ household_amenities)\n",
      "2. Final dataset: 77 districts × 41 features\n",
      "3. Expected: 77 districts of Nepal\n",
      "4. Data saved to 'data/processed/' folder\n",
      "\n",
      "Now run 03_feature_engineering.py to create vulnerability features!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "QUICK DATA CLEANING - Robust version for all 77 districts\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup paths\n",
    "RAW_PATH = Path(r'C:\\Users\\saurav\\Downloads\\SEVI_Nepal_Project\\data\\raw')\n",
    "PROCESSED_PATH = Path(r'C:\\Users\\saurav\\Downloads\\SEVI_Nepal_Project\\data\\processed')\n",
    "PROCESSED_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ROBUST DATA EXTRACTION - 77 DISTRICTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Identify the correct file names\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nChecking available files...\")\n",
    "all_files = list(RAW_PATH.glob('*.csv'))\n",
    "print(f\"Found {len(all_files)} CSV files\")\n",
    "\n",
    "# Show all files\n",
    "for i, file_path in enumerate(all_files, 1):\n",
    "    print(f\"{i:2}. {file_path.name}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Load cooking_fuel to get district IDs\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYZING cooking_fuel.csv FOR DISTRICT IDS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    cf_df = pd.read_csv(RAW_PATH / 'cooking_fuel.csv')\n",
    "    print(f\"File shape: {cf_df.shape}\")\n",
    "    print(f\"Columns: {list(cf_df.columns)}\")\n",
    "    \n",
    "    # Check structure\n",
    "    print(\"\\nFirst 25 rows:\")\n",
    "    for i, row in cf_df.head(25).iterrows():\n",
    "        area_name = str(row.get('AREA', row.get('AREA_NAME', 'Unknown')))\n",
    "        print(f\"ID {row['ID']:3}: {area_name}\")\n",
    "    \n",
    "    # Analyze ID structure\n",
    "    print(f\"\\nID analysis:\")\n",
    "    print(f\"  Min ID: {cf_df['ID'].min()}\")\n",
    "    print(f\"  Max ID: {cf_df['ID'].max()}\")\n",
    "    print(f\"  Unique IDs: {cf_df['ID'].nunique()}\")\n",
    "    \n",
    "    # Identify different sections\n",
    "    print(f\"\\nIdentifying sections:\")\n",
    "    \n",
    "    # IDs 1-6: National aggregates\n",
    "    national = cf_df[cf_df['ID'] <= 6]\n",
    "    print(f\"  National aggregates (ID 1-6): {len(national)} rows\")\n",
    "    \n",
    "    # IDs 7-13: Provinces\n",
    "    provinces = cf_df[(cf_df['ID'] >= 7) & (cf_df['ID'] <= 13)]\n",
    "    print(f\"  Provinces (ID 7-13): {len(provinces)} rows\")\n",
    "    if len(provinces) > 0:\n",
    "        print(f\"    Province names: {list(provinces['AREA'].unique())}\")\n",
    "    \n",
    "    # IDs 14+: Districts\n",
    "    districts = cf_df[cf_df['ID'] >= 14]\n",
    "    print(f\"  Districts (ID 14+): {len(districts)} rows\")\n",
    "    \n",
    "    if len(districts) == 77:\n",
    "        print(f\"  ✓ Perfect! Found all 77 districts\")\n",
    "    elif len(districts) > 77:\n",
    "        print(f\"  ⚠ Found {len(districts)} rows, expected 77\")\n",
    "        # Check for duplicates or other issues\n",
    "        duplicate_ids = districts[districts.duplicated('ID', keep=False)]\n",
    "        if len(duplicate_ids) > 0:\n",
    "            print(f\"  ⚠ Found {len(duplicate_ids)} duplicate IDs\")\n",
    "    else:\n",
    "        print(f\"  ⚠ Only found {len(districts)} districts, expected 77\")\n",
    "    \n",
    "    # Create district reference\n",
    "    district_ref = districts[['ID', 'AREA']].copy()\n",
    "    district_ref = district_ref.rename(columns={'AREA': 'DISTRICT_NAME'})\n",
    "    district_ref = district_ref.sort_values('ID').reset_index(drop=True)\n",
    "    \n",
    "    # Save district reference\n",
    "    district_ref.to_csv(PROCESSED_PATH / 'district_reference.csv', index=False)\n",
    "    print(f\"\\n✓ District reference saved with {len(district_ref)} districts\")\n",
    "    \n",
    "    # Display first and last districts\n",
    "    print(f\"\\nFirst 5 districts:\")\n",
    "    print(district_ref.head().to_string(index=False))\n",
    "    print(f\"\\nLast 5 districts:\")\n",
    "    print(district_ref.tail().to_string(index=False))\n",
    "    \n",
    "    # District IDs list\n",
    "    district_ids = district_ref['ID'].tolist()\n",
    "    print(f\"\\nDistrict ID range: {min(district_ids)} to {max(district_ids)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR with cooking_fuel.csv: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Check key files for district data\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CHECKING KEY FILES FOR DISTRICT DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define files to check - updated based on what we found\n",
    "key_files_to_check = [\n",
    "    'wall_materials.csv',\n",
    "    'drinking_watersource.csv', \n",
    "    'toilet_facility.csv',  # or toilet_facilities.csv?\n",
    "    'cooking_fuel.csv',     # Already processed\n",
    "    'household_amenities.csv',\n",
    "    'main_source_of_lighting.csv',\n",
    "    'radio_television_etc.csv'\n",
    "]\n",
    "\n",
    "file_info = {}\n",
    "\n",
    "for filename in key_files_to_check:\n",
    "    file_path = RAW_PATH / filename\n",
    "    if file_path.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            file_info[filename] = {\n",
    "                'shape': df.shape,\n",
    "                'columns': list(df.columns),\n",
    "                'has_id': 'ID' in df.columns,\n",
    "                'has_area': any(col in df.columns for col in ['AREA', 'AREA_NAME', 'DISTRICT']),\n",
    "                'district_rows': 0\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{filename}:\")\n",
    "            print(f\"  Shape: {df.shape[0]} rows × {df.shape[1]} cols\")\n",
    "            print(f\"  Has ID: {'Yes' if 'ID' in df.columns else 'No'}\")\n",
    "            \n",
    "            if 'ID' in df.columns:\n",
    "                # Check district rows\n",
    "                district_rows = df[df['ID'].isin(district_ids)]\n",
    "                count = len(district_rows)\n",
    "                file_info[filename]['district_rows'] = count\n",
    "                print(f\"  District rows: {count}\")\n",
    "                \n",
    "                if count != 77:\n",
    "                    print(f\"  ⚠ Expected 77, found {count}\")\n",
    "                    \n",
    "            # Check if LONG format\n",
    "            area_col = None\n",
    "            for col in ['AREA', 'AREA_NAME', 'DISTRICT']:\n",
    "                if col in df.columns:\n",
    "                    area_col = col\n",
    "                    break\n",
    "            \n",
    "            if area_col:\n",
    "                unique_areas = df[area_col].nunique()\n",
    "                if df.shape[0] > unique_areas:\n",
    "                    print(f\"  ⚠ LONG format: {df.shape[0]} rows for {unique_areas} areas\")\n",
    "                    print(f\"    ~{df.shape[0]/unique_areas:.1f} rows per area\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR: {e}\")\n",
    "    else:\n",
    "        print(f\"\\n{filename}: FILE NOT FOUND\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Create merged dataset - ROBUST VERSION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING MERGED DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Start with district reference\n",
    "merged_df = district_ref.copy()\n",
    "print(f\"Starting with {len(merged_df)} districts\")\n",
    "\n",
    "# Files to merge (with priority)\n",
    "merge_priority = [\n",
    "    'wall_materials.csv',\n",
    "    'drinking_watersource.csv',\n",
    "    'toilet_facility.csv',  # Try without 's'\n",
    "    'main_source_of_lighting.csv',\n",
    "    'radio_television_etc.csv'\n",
    "]\n",
    "\n",
    "# Also try alternative names\n",
    "try_alt_names = {\n",
    "    'toilet_facility.csv': 'toilet_facilities.csv',\n",
    "    'radio_television_etc.csv': 'radio_television_etc..csv'  # Sometimes has double extension\n",
    "}\n",
    "\n",
    "for filename in merge_priority:\n",
    "    actual_file = filename\n",
    "    \n",
    "    # Try alternative name if main doesn't exist\n",
    "    if not (RAW_PATH / filename).exists() and filename in try_alt_names:\n",
    "        alt_name = try_alt_names[filename]\n",
    "        if (RAW_PATH / alt_name).exists():\n",
    "            actual_file = alt_name\n",
    "            print(f\"  Using alternative: {alt_name}\")\n",
    "    \n",
    "    file_path = RAW_PATH / actual_file\n",
    "    if file_path.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            if 'ID' in df.columns:\n",
    "                # Filter for districts\n",
    "                df_districts = df[df['ID'].isin(district_ids)].copy()\n",
    "                \n",
    "                if len(df_districts) > 0:\n",
    "                    # Get area column name\n",
    "                    area_col = None\n",
    "                    for col in ['AREA', 'AREA_NAME', 'DISTRICT']:\n",
    "                        if col in df.columns:\n",
    "                            area_col = col\n",
    "                            break\n",
    "                    \n",
    "                    # Remove area column if present (we already have it from cooking_fuel)\n",
    "                    cols_to_merge = []\n",
    "                    for col in df_districts.columns:\n",
    "                        if col != 'ID' and col != area_col and not col.startswith('Unnamed'):\n",
    "                            cols_to_merge.append(col)\n",
    "                    \n",
    "                    if cols_to_merge:\n",
    "                        # Create prefix from filename (first 3 chars)\n",
    "                        prefix = actual_file[:3].upper().replace('.', '')\n",
    "                        rename_dict = {}\n",
    "                        for col in cols_to_merge:\n",
    "                            # Handle duplicates\n",
    "                            new_name = f\"{prefix}_{col}\"\n",
    "                            rename_dict[col] = new_name\n",
    "                        \n",
    "                        # Prepare for merge\n",
    "                        df_to_merge = df_districts[['ID'] + cols_to_merge].rename(columns=rename_dict)\n",
    "                        \n",
    "                        # Check for duplicate columns\n",
    "                        duplicate_cols = set(df_to_merge.columns) & set(merged_df.columns)\n",
    "                        if duplicate_cols:\n",
    "                            print(f\"  ⚠ Duplicate columns: {duplicate_cols}\")\n",
    "                            # Remove ID from duplicates\n",
    "                            duplicate_cols = [col for col in duplicate_cols if col != 'ID']\n",
    "                        \n",
    "                        before_cols = len(merged_df.columns)\n",
    "                        merged_df = pd.merge(merged_df, df_to_merge, on='ID', how='left', suffixes=('', '_dup'))\n",
    "                        after_cols = len(merged_df.columns)\n",
    "                        \n",
    "                        added = after_cols - before_cols\n",
    "                        print(f\"  ✓ {actual_file}: Added {added} columns\")\n",
    "                        \n",
    "                        # Remove duplicate columns that might have been created\n",
    "                        dup_suffix = [col for col in merged_df.columns if col.endswith('_dup')]\n",
    "                        if dup_suffix:\n",
    "                            merged_df = merged_df.drop(columns=dup_suffix)\n",
    "                    else:\n",
    "                        print(f\"  ⚠ {actual_file}: No columns to merge\")\n",
    "                else:\n",
    "                    print(f\"  ⚠ {actual_file}: No district rows found\")\n",
    "            else:\n",
    "                print(f\"  ⚠ {actual_file}: No ID column, skipping for now\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR processing {actual_file}: {e}\")\n",
    "    else:\n",
    "        print(f\"  ⚠ {filename}: File not found\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: Handle household_amenities separately (no ID column)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROCESSING household_amenities.csv\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "ha_path = RAW_PATH / 'household_amenities.csv'\n",
    "if ha_path.exists():\n",
    "    try:\n",
    "        ha_df = pd.read_csv(ha_path)\n",
    "        print(f\"  Shape: {ha_df.shape}\")\n",
    "        print(f\"  Columns: {ha_df.columns.tolist()}\")\n",
    "        \n",
    "        # This file has AREA_NAME and AREA_TYPE\n",
    "        if 'AREA_NAME' in ha_df.columns and 'AREA_TYPE' in ha_df.columns:\n",
    "            # Filter for district rows (not Country, Urban/Rural, Ecological Belt)\n",
    "            ha_districts = ha_df[\n",
    "                (~ha_df['AREA_TYPE'].isin(['Country', 'Ecological Belt'])) & \n",
    "                (ha_df['AREA_TYPE'] != 'Urban/Rural')\n",
    "            ].copy()\n",
    "            \n",
    "            print(f\"  District rows: {len(ha_districts)}\")\n",
    "            \n",
    "            if len(ha_districts) > 0:\n",
    "                # Get columns to merge (exclude metadata)\n",
    "                exclude_cols = ['AREA_NAME', 'AREA_TYPE']\n",
    "                cols_to_merge = [col for col in ha_districts.columns \n",
    "                               if col not in exclude_cols and not col.startswith('Unnamed')]\n",
    "                \n",
    "                if cols_to_merge:\n",
    "                    # Add prefix\n",
    "                    rename_dict = {col: f\"HA_{col}\" for col in cols_to_merge}\n",
    "                    \n",
    "                    # Prepare for merge by name\n",
    "                    df_to_merge = ha_districts[['AREA_NAME'] + cols_to_merge].rename(\n",
    "                        columns={'AREA_NAME': 'DISTRICT_NAME', **rename_dict}\n",
    "                    )\n",
    "                    \n",
    "                    # Clean names for matching\n",
    "                    merged_df['DISTRICT_NAME_CLEAN'] = merged_df['DISTRICT_NAME'].str.strip().str.lower()\n",
    "                    df_to_merge['DISTRICT_NAME_CLEAN'] = df_to_merge['DISTRICT_NAME'].str.strip().str.lower()\n",
    "                    \n",
    "                    # Merge\n",
    "                    before_cols = len(merged_df.columns)\n",
    "                    merged_df = pd.merge(merged_df, df_to_merge.drop('DISTRICT_NAME', axis=1), \n",
    "                                       on='DISTRICT_NAME_CLEAN', how='left')\n",
    "                    added = len(merged_df.columns) - before_cols\n",
    "                    \n",
    "                    # Clean up\n",
    "                    merged_df = merged_df.drop('DISTRICT_NAME_CLEAN', axis=1)\n",
    "                    \n",
    "                    print(f\"  ✓ Added {added} columns from household_amenities\")\n",
    "                else:\n",
    "                    print(f\"  ⚠ No columns to merge\")\n",
    "            else:\n",
    "                print(f\"  ⚠ No district rows found\")\n",
    "        else:\n",
    "            print(f\"  ⚠ Missing AREA_NAME or AREA_TYPE columns\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: {e}\")\n",
    "else:\n",
    "    print(f\"  ⚠ File not found\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: Final checks and save\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nDataset shape: {merged_df.shape}\")\n",
    "print(f\"Districts: {len(merged_df)}\")\n",
    "print(f\"Total features: {len(merged_df.columns)}\")\n",
    "\n",
    "# Check for missing districts\n",
    "if len(merged_df) != 77:\n",
    "    print(f\"\\n⚠ WARNING: Expected 77 districts, found {len(merged_df)}\")\n",
    "    \n",
    "    # Find missing IDs\n",
    "    missing_ids = set(district_ids) - set(merged_df['ID'].tolist())\n",
    "    if missing_ids:\n",
    "        print(f\"  Missing district IDs: {sorted(missing_ids)}\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = merged_df[merged_df.duplicated('ID', keep=False)]\n",
    "if len(duplicates) > 0:\n",
    "    print(f\"⚠ WARNING: Found {len(duplicates)} duplicate district IDs\")\n",
    "    print(duplicates[['ID', 'DISTRICT_NAME']].head())\n",
    "\n",
    "# Check missing values\n",
    "missing = merged_df.isnull().sum()\n",
    "missing_cols = missing[missing > 0]\n",
    "if len(missing_cols) > 0:\n",
    "    print(f\"\\nColumns with missing values: {len(missing_cols)}\")\n",
    "    print(\"Top 5 columns with most missing:\")\n",
    "    for col, count in missing_cols.sort_values(ascending=False).head(5).items():\n",
    "        pct = count / len(merged_df) * 100\n",
    "        print(f\"  {col:30}: {count:3} ({pct:.1f}%)\")\n",
    "\n",
    "# Save the merged dataset\n",
    "merged_path = PROCESSED_PATH / 'simple_merged_districts.csv'\n",
    "merged_df.to_csv(merged_path, index=False)\n",
    "print(f\"\\n✓ Merged dataset saved to: {merged_path}\")\n",
    "\n",
    "# Also save a sample\n",
    "sample_path = PROCESSED_PATH / 'district_sample.csv'\n",
    "merged_df.head(10).to_csv(sample_path, index=False)\n",
    "print(f\"✓ Sample saved to: {sample_path}\")\n",
    "\n",
    "# Display structure\n",
    "print(f\"\\nDataset structure:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Column':40} {'Type':10} {'Non-Null':10}\")\n",
    "print(\"-\" * 40)\n",
    "for col in merged_df.columns:\n",
    "    non_null = merged_df[col].notna().sum()\n",
    "    dtype = str(merged_df[col].dtype)\n",
    "    print(f\"{col:40} {dtype:10} {non_null:10}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"1. Processed files: {len(merge_priority) + 1} (+ household_amenities)\")\n",
    "print(f\"2. Final dataset: {len(merged_df)} districts × {len(merged_df.columns)} features\")\n",
    "print(f\"3. Expected: 77 districts of Nepal\")\n",
    "print(f\"4. Data saved to 'data/processed/' folder\")\n",
    "print(\"\\nNow run 03_feature_engineering.py to create vulnerability features!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
