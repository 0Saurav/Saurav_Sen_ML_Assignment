{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ffa3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA CLEANING - FINAL CORRECTED VERSION\n",
      "================================================================================\n",
      "Found 16 CSV files\n",
      "\n",
      "below_secondary_education       222 rows ×  24 cols\n",
      "children_living_arrangement     231 rows ×  14 cols\n",
      "cooking_fuel                     90 rows ×   9 cols\n",
      "drinking_watersource             90 rows ×  11 cols\n",
      "educational_attainment          270 rows ×  16 cols\n",
      "educational_field_distribution  267 rows ×  18 cols\n",
      "floor_type                       90 rows ×   8 cols\n",
      "foundation_type                  90 rows ×   7 cols\n",
      "household_amenities              90 rows ×  19 cols\n",
      "housing_ownership                90 rows ×   7 cols\n",
      "lighting_source                  90 rows ×   7 cols\n",
      "months_worked                   270 rows ×   9 cols\n",
      "population_occupation           270 rows ×  16 cols\n",
      "roof_type                        90 rows ×  10 cols\n",
      "toilet_facility                  90 rows ×   7 cols\n",
      "wall_materials                   90 rows ×  10 cols\n",
      "\n",
      "================================================================================\n",
      "STANDARDIZING DISTRICT IDS\n",
      "================================================================================\n",
      "\n",
      "Column names in cooking_fuel.csv:\n",
      "['ID', 'AREA', 'WOOD_FIREWOOD', 'LPG_GAS', 'ELECTRICITY', 'COW_DUNG', 'BIOGAS', 'KEROSENE', 'OTHER']\n",
      "\n",
      "Using columns: ID as ID, AREA as district name\n",
      "\n",
      "✓ Found 77 districts\n",
      "✓ District ID range: 14 to 90\n",
      "✓ Perfect! All 77 districts found\n",
      "\n",
      "First 10 districts:\n",
      " ID DISTRICT_NAME\n",
      " 14     Taplejung\n",
      " 15 Sankhuwasabha\n",
      " 16    Solukhumbu\n",
      " 17   Okhaldhunga\n",
      " 18       Khotang\n",
      " 19       Bhojpur\n",
      " 20      Dhankuta\n",
      " 21     Terhathum\n",
      " 22     Panchthar\n",
      " 23          Ilam\n",
      "\n",
      "✓ District reference saved\n",
      "\n",
      "================================================================================\n",
      "PROCESSING ALL FILES\n",
      "================================================================================\n",
      "\n",
      "Processing: below_secondary_education\n",
      "  Shape: (222, 24)\n",
      "  Found 77 district rows\n",
      "\n",
      "Processing: children_living_arrangement\n",
      "  Shape: (231, 14)\n",
      "  Found 77 district rows\n",
      "\n",
      "Processing: cooking_fuel\n",
      "  Shape: (90, 9)\n",
      "  Found 77 district rows\n",
      "\n",
      "Processing: drinking_watersource\n",
      "  Shape: (90, 11)\n",
      "  Found 77 district rows\n",
      "\n",
      "Processing: educational_attainment\n",
      "  Shape: (270, 16)\n",
      "  Found 77 district rows\n",
      "\n",
      "Processing: educational_field_distribution\n",
      "  Shape: (267, 18)\n",
      "  Found 77 district rows\n",
      "\n",
      "Processing: floor_type\n",
      "  Shape: (90, 8)\n",
      "  Found 77 district rows\n",
      "\n",
      "Processing: foundation_type\n",
      "  Shape: (90, 7)\n",
      "  Found 77 district rows\n",
      "\n",
      "Processing: household_amenities\n",
      "  Shape: (90, 19)\n",
      "  File has AREA_NAME column instead of ID\n",
      "  Will need to merge by name later\n",
      "\n",
      "Processing: housing_ownership\n",
      "  Shape: (90, 7)\n",
      "  Found 77 district rows\n",
      "\n",
      "Processing: lighting_source\n",
      "  Shape: (90, 7)\n",
      "  Found 77 district rows\n",
      "\n",
      "Processing: months_worked\n",
      "  Shape: (270, 9)\n",
      "  Found 77 district rows\n",
      "\n",
      "Processing: population_occupation\n",
      "  Shape: (270, 16)\n",
      "  Found 77 district rows\n",
      "\n",
      "Processing: roof_type\n",
      "  Shape: (90, 10)\n",
      "  Found 77 district rows\n",
      "\n",
      "Processing: toilet_facility\n",
      "  Shape: (90, 7)\n",
      "  Found 77 district rows\n",
      "\n",
      "Processing: wall_materials\n",
      "  Shape: (90, 10)\n",
      "  Found 77 district rows\n",
      "\n",
      "✓ Processed 16 files\n",
      "  Files with ID column: 15\n",
      "  Files without ID column: 1\n",
      "\n",
      "================================================================================\n",
      "MERGING ALL DISTRICT DATA\n",
      "================================================================================\n",
      "Starting base: 77 districts\n",
      "\n",
      "Merging files with ID columns:\n",
      "  below_secondary_education + 19 columns\n",
      "  children_living_arrangement +  9 columns\n",
      "  drinking_watersource      +  8 columns\n",
      "  educational_attainment    + 12 columns\n",
      "  educational_field_distribution + 13 columns\n",
      "  floor_type                +  6 columns\n",
      "  foundation_type           +  5 columns\n",
      "  housing_ownership         +  5 columns\n",
      "  lighting_source           +  5 columns\n",
      "  months_worked             +  4 columns\n",
      "  population_occupation     + 11 columns\n",
      "  roof_type                 +  8 columns\n",
      "  toilet_facility           +  5 columns\n",
      "  wall_materials            +  8 columns\n",
      "\n",
      "Merging files without ID columns:\n",
      "  household_amenities       + 17 columns\n",
      "\n",
      "Final dataset shape: (77, 137)\n",
      "Districts: 77\n",
      "Total features: 137\n",
      "\n",
      "================================================================================\n",
      "DATA QUALITY CHECKS\n",
      "================================================================================\n",
      "✓ No duplicate district IDs\n",
      "✓ All 77 districts present\n",
      "\n",
      "⚠ Columns with missing values: 17\n",
      "  Overall missing data: 0.8%\n",
      "\n",
      "Column naming patterns:\n",
      "  Unique prefixes: 14\n",
      "  Most common prefixes:\n",
      "    EDUC       :  25 columns\n",
      "    HOUS       :  22 columns\n",
      "    BELO       :  19 columns\n",
      "    POPU       :  11 columns\n",
      "    CHIL       :   9 columns\n",
      "    DRIN       :   8 columns\n",
      "    ROOF       :   8 columns\n",
      "    WALL       :   8 columns\n",
      "    FLOO       :   6 columns\n",
      "    FOUN       :   5 columns\n",
      "\n",
      "================================================================================\n",
      "SAVING CLEANED DATA\n",
      "================================================================================\n",
      "✓ Cleaned data saved to: C:\\Users\\saurav\\Downloads\\SEVI_Nepal_Project\\data\\processed\\nepal_districts_cleaned.csv\n",
      "✓ District list saved\n",
      "✓ Summary saved to: C:\\Users\\saurav\\Downloads\\SEVI_Nepal_Project\\data\\processed\\cleaning_summary.txt\n",
      "\n",
      "================================================================================\n",
      "SCRIPT COMPLETE - READY FOR FEATURE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "NEXT STEP:\n",
      "1. Open 03_feature_engineering.py\n",
      "2. Key tasks:\n",
      "   - Convert counts to percentages\n",
      "   - Create vulnerability indices\n",
      "   - Handle any remaining missing data\n",
      "\n",
      "Your cleaned data is ready at: C:\\Users\\saurav\\Downloads\\SEVI_Nepal_Project\\data\\processed\\nepal_districts_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Nepal District Vulnerability Analysis - Data Cleaning - FINAL CORRECTED VERSION\n",
    "Step 2: Clean and merge district-level data\n",
    "Author: [Your Name]\n",
    "Date: [Today's Date]\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# Setup paths\n",
    "RAW_PATH = Path(r'C:\\Users\\saurav\\Downloads\\SEVI_Nepal_Project\\data\\raw')\n",
    "PROCESSED_PATH = Path(r'C:\\Users\\saurav\\Downloads\\SEVI_Nepal_Project\\data\\processed')\n",
    "PROCESSED_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA CLEANING - FINAL CORRECTED VERSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Load all files\n",
    "# ============================================================================\n",
    "\n",
    "def load_all_files():\n",
    "    \"\"\"Load all CSV files with consistent encoding\"\"\"\n",
    "    csv_files = list(RAW_PATH.glob('*.csv'))\n",
    "    print(f\"Found {len(csv_files)} CSV files\\n\")\n",
    "    \n",
    "    dataframes = {}\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            # Try different encodings if needed\n",
    "            try:\n",
    "                df = pd.read_csv(csv_file, encoding='utf-8')\n",
    "            except:\n",
    "                df = pd.read_csv(csv_file, encoding='latin-1')\n",
    "                \n",
    "            dataframes[csv_file.stem] = df\n",
    "            print(f\"{csv_file.stem:30} {df.shape[0]:4} rows × {df.shape[1]:3} cols\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading {csv_file.name}: {e}\")\n",
    "    \n",
    "    return dataframes\n",
    "\n",
    "dataframes = load_all_files()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Standardize district IDs from cooking_fuel.csv\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STANDARDIZING DISTRICT IDS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# GLOBAL VARIABLE - will be used by all functions\n",
    "DISTRICT_IDS = None\n",
    "DISTRICT_REF = None\n",
    "\n",
    "if 'cooking_fuel' in dataframes:\n",
    "    cf_df = dataframes['cooking_fuel']\n",
    "    \n",
    "    # Standardize column names\n",
    "    cf_df.columns = [col.strip().upper() for col in cf_df.columns]\n",
    "    \n",
    "    print(\"\\nColumn names in cooking_fuel.csv:\")\n",
    "    print(cf_df.columns.tolist())\n",
    "    \n",
    "    # Find correct ID and name columns\n",
    "    id_col = None\n",
    "    name_col = None\n",
    "    \n",
    "    for col in cf_df.columns:\n",
    "        if col == 'ID' or 'ID_' in col:\n",
    "            id_col = col\n",
    "        elif col in ['AREA', 'AREA_NAME', 'NAME', 'DISTRICT']:\n",
    "            name_col = col\n",
    "    \n",
    "    if not id_col or not name_col:\n",
    "        print(\"ERROR: Could not find ID or name column in cooking_fuel.csv\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(f\"\\nUsing columns: {id_col} as ID, {name_col} as district name\")\n",
    "    \n",
    "    # Extract districts (IDs 14 and above for Nepal districts)\n",
    "    DISTRICT_REF = cf_df[cf_df[id_col] >= 14].copy()\n",
    "    DISTRICT_REF = DISTRICT_REF[[id_col, name_col]].copy()\n",
    "    DISTRICT_REF.columns = ['ID', 'DISTRICT_NAME']\n",
    "    DISTRICT_REF = DISTRICT_REF.sort_values('ID').reset_index(drop=True)\n",
    "    \n",
    "    # Clean district names\n",
    "    DISTRICT_REF['DISTRICT_NAME'] = DISTRICT_REF['DISTRICT_NAME'].str.strip()\n",
    "    \n",
    "    # Create district ID list\n",
    "    DISTRICT_IDS = DISTRICT_REF['ID'].tolist()\n",
    "    \n",
    "    print(f\"\\n✓ Found {len(DISTRICT_REF)} districts\")\n",
    "    print(f\"✓ District ID range: {min(DISTRICT_IDS)} to {max(DISTRICT_IDS)}\")\n",
    "    \n",
    "    if len(DISTRICT_REF) == 77:\n",
    "        print(\"✓ Perfect! All 77 districts found\")\n",
    "    else:\n",
    "        print(f\"⚠ Warning: Expected 77 districts, found {len(DISTRICT_REF)}\")\n",
    "    \n",
    "    print(\"\\nFirst 10 districts:\")\n",
    "    print(DISTRICT_REF.head(10).to_string(index=False))\n",
    "    \n",
    "    # Save district reference\n",
    "    DISTRICT_REF.to_csv(PROCESSED_PATH / 'district_reference.csv', index=False)\n",
    "    print(f\"\\n✓ District reference saved\")\n",
    "else:\n",
    "    print(\"ERROR: cooking_fuel.csv not found - needed for district IDs\")\n",
    "    exit(1)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Function to extract districts from any file\n",
    "# ============================================================================\n",
    "\n",
    "def clean_column_names(df):\n",
    "    \"\"\"Standardize column names\"\"\"\n",
    "    df.columns = [col.strip().upper() for col in df.columns]\n",
    "    return df\n",
    "\n",
    "def extract_districts(df, file_name):\n",
    "    \"\"\"Extract district rows from any dataframe\"\"\"\n",
    "    print(f\"\\nProcessing: {file_name}\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    \n",
    "    # Clean column names first\n",
    "    df = clean_column_names(df.copy())\n",
    "    \n",
    "    # Case 1: File has ID column\n",
    "    if 'ID' in df.columns:\n",
    "        # Filter for district IDs\n",
    "        district_df = df[df['ID'].isin(DISTRICT_IDS)].copy()\n",
    "        \n",
    "        if len(district_df) == 0:\n",
    "            print(f\"  ⚠ No districts found with standard IDs\")\n",
    "            print(f\"  Trying alternative: ID >= 14\")\n",
    "            district_df = df[df['ID'] >= 14].copy()\n",
    "        \n",
    "        print(f\"  Found {len(district_df)} district rows\")\n",
    "        \n",
    "        # Standardize name column\n",
    "        name_col = None\n",
    "        for col in df.columns:\n",
    "            if col in ['AREA', 'AREA_NAME', 'NAME', 'DISTRICT']:\n",
    "                name_col = col\n",
    "                break\n",
    "        \n",
    "        if name_col and name_col != 'DISTRICT_NAME':\n",
    "            district_df = district_df.rename(columns={name_col: 'DISTRICT_NAME'})\n",
    "            \n",
    "        return district_df\n",
    "    \n",
    "    # Case 2: File has AREA_NAME or similar (like household_amenities)\n",
    "    elif 'AREA_NAME' in df.columns or 'DISTRICT' in df.columns:\n",
    "        name_col = 'AREA_NAME' if 'AREA_NAME' in df.columns else 'DISTRICT'\n",
    "        \n",
    "        print(f\"  File has {name_col} column instead of ID\")\n",
    "        print(f\"  Will need to merge by name later\")\n",
    "        \n",
    "        # Clean names for consistency\n",
    "        df = df.copy()\n",
    "        df[name_col] = df[name_col].str.strip()\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    # Case 3: Cannot identify districts\n",
    "    else:\n",
    "        print(f\"  ⚠ Cannot identify districts - no ID or name column\")\n",
    "        return None\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Process all files\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROCESSING ALL FILES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "district_dfs = {}\n",
    "files_with_ids = []\n",
    "files_without_ids = []\n",
    "\n",
    "for file_name, df in dataframes.items():\n",
    "    result = extract_districts(df, file_name)\n",
    "    \n",
    "    if result is not None:\n",
    "        district_dfs[file_name] = result\n",
    "        \n",
    "        if 'ID' in result.columns:\n",
    "            files_with_ids.append(file_name)\n",
    "        else:\n",
    "            files_without_ids.append(file_name)\n",
    "\n",
    "print(f\"\\n✓ Processed {len(district_dfs)} files\")\n",
    "print(f\"  Files with ID column: {len(files_with_ids)}\")\n",
    "print(f\"  Files without ID column: {len(files_without_ids)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: Merge all data - FIXED VERSION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MERGING ALL DISTRICT DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Start with district reference as base\n",
    "merged_df = DISTRICT_REF.copy()\n",
    "print(f\"Starting base: {len(merged_df)} districts\")\n",
    "\n",
    "# Function to create safe column names\n",
    "def create_column_prefix(file_name):\n",
    "    \"\"\"Create short prefix from filename\"\"\"\n",
    "    # Remove special characters, take first 3-4 chars\n",
    "    clean_name = re.sub(r'[^a-zA-Z]', '', file_name)\n",
    "    prefix = clean_name[:4].upper() if len(clean_name) >= 4 else clean_name.upper()\n",
    "    return prefix\n",
    "\n",
    "# Merge files WITH ID columns first\n",
    "print(\"\\nMerging files with ID columns:\")\n",
    "for file_name in files_with_ids:\n",
    "    if file_name != 'cooking_fuel':  # Already have district names from this\n",
    "        df = district_dfs[file_name]\n",
    "        \n",
    "        # Get columns to merge (exclude metadata)\n",
    "        exclude_patterns = ['ID', 'DISTRICT_NAME', 'SEX', 'AREA_LEVEL', \n",
    "                           'AREA_TYPE', 'TOTAL_', 'AREA_']\n",
    "        cols_to_merge = []\n",
    "        \n",
    "        for col in df.columns:\n",
    "            if col not in ['ID', 'DISTRICT_NAME'] and not any(pattern in col for pattern in exclude_patterns):\n",
    "                cols_to_merge.append(col)\n",
    "        \n",
    "        if cols_to_merge:\n",
    "            # Create unique column names\n",
    "            prefix = create_column_prefix(file_name)\n",
    "            rename_dict = {col: f\"{prefix}_{col}\" for col in cols_to_merge}\n",
    "            \n",
    "            # Prepare DataFrame for merge\n",
    "            df_to_merge = df[['ID'] + cols_to_merge].rename(columns=rename_dict)\n",
    "            \n",
    "            # Merge\n",
    "            before_cols = len(merged_df.columns)\n",
    "            merged_df = pd.merge(merged_df, df_to_merge, on='ID', how='left')\n",
    "            added_cols = len(merged_df.columns) - before_cols\n",
    "            \n",
    "            print(f\"  {file_name:25} +{added_cols:3} columns\")\n",
    "\n",
    "# Handle files WITHOUT ID columns (merge by name)\n",
    "print(\"\\nMerging files without ID columns:\")\n",
    "for file_name in files_without_ids:\n",
    "    df = district_dfs[file_name]\n",
    "    \n",
    "    # Find name column\n",
    "    name_col = None\n",
    "    for col in ['AREA_NAME', 'DISTRICT']:\n",
    "        if col in df.columns:\n",
    "            name_col = col\n",
    "            break\n",
    "    \n",
    "    if name_col:\n",
    "        # Clean and standardize names\n",
    "        df = df.copy()\n",
    "        df[name_col] = df[name_col].str.strip().str.upper()\n",
    "        \n",
    "        # Filter for likely district rows (not aggregates)\n",
    "        # Districts usually don't have these keywords\n",
    "        exclude_keywords = ['TOTAL', 'NATIONAL', 'PROVINCE', 'REGION', 'URBAN', 'RURAL']\n",
    "        mask = ~df[name_col].str.contains('|'.join(exclude_keywords), case=False, na=False)\n",
    "        district_rows = df[mask].copy()\n",
    "        \n",
    "        if len(district_rows) > 0:\n",
    "            # Get data columns\n",
    "            exclude_cols = [name_col, 'AREA_TYPE', 'TOTAL']\n",
    "            cols_to_merge = [col for col in district_rows.columns \n",
    "                           if col not in exclude_cols and not col.startswith('UNNAMED')]\n",
    "            \n",
    "            if cols_to_merge:\n",
    "                # Create unique column names\n",
    "                prefix = create_column_prefix(file_name)\n",
    "                rename_dict = {col: f\"{prefix}_{col}\" for col in cols_to_merge}\n",
    "                \n",
    "                # Prepare for merge\n",
    "                df_to_merge = district_rows[[name_col] + cols_to_merge].copy()\n",
    "                df_to_merge = df_to_merge.rename(columns={name_col: 'DISTRICT_NAME', **rename_dict})\n",
    "                \n",
    "                # Standardize district names for merge\n",
    "                merged_df['DISTRICT_NAME_UPPER'] = merged_df['DISTRICT_NAME'].str.upper()\n",
    "                df_to_merge['DISTRICT_NAME_UPPER'] = df_to_merge['DISTRICT_NAME'].str.upper()\n",
    "                \n",
    "                # Merge\n",
    "                before_cols = len(merged_df.columns)\n",
    "                merged_df = pd.merge(merged_df, df_to_merge.drop('DISTRICT_NAME', axis=1), \n",
    "                                   on='DISTRICT_NAME_UPPER', how='left')\n",
    "                added_cols = len(merged_df.columns) - before_cols\n",
    "                \n",
    "                # Clean up\n",
    "                merged_df = merged_df.drop('DISTRICT_NAME_UPPER', axis=1)\n",
    "                \n",
    "                print(f\"  {file_name:25} +{added_cols:3} columns\")\n",
    "        else:\n",
    "            print(f\"  {file_name:25} No district rows found\")\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {merged_df.shape}\")\n",
    "print(f\"Districts: {len(merged_df)}\")\n",
    "print(f\"Total features: {len(merged_df.columns)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: Data quality checks and save - IMPROVED\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA QUALITY CHECKS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Check for duplicate districts\n",
    "duplicate_ids = merged_df[merged_df.duplicated('ID', keep=False)]\n",
    "if len(duplicate_ids) > 0:\n",
    "    print(f\"⚠ WARNING: Found {len(duplicate_ids)} duplicate district IDs\")\n",
    "    print(duplicate_ids[['ID', 'DISTRICT_NAME']].head())\n",
    "else:\n",
    "    print(\"✓ No duplicate district IDs\")\n",
    "\n",
    "# 2. Check all 77 districts are present\n",
    "if len(merged_df) == 77:\n",
    "    print(\"✓ All 77 districts present\")\n",
    "else:\n",
    "    print(f\"⚠ Missing districts: Expected 77, found {len(merged_df)}\")\n",
    "    missing_ids = set(DISTRICT_IDS) - set(merged_df['ID'].tolist())\n",
    "    if missing_ids:\n",
    "        print(f\"  Missing IDs: {sorted(missing_ids)}\")\n",
    "\n",
    "# 3. Check for missing values\n",
    "missing = merged_df.isnull().sum()\n",
    "missing_cols = missing[missing > 0]\n",
    "\n",
    "if len(missing_cols) > 0:\n",
    "    print(f\"\\n⚠ Columns with missing values: {len(missing_cols)}\")\n",
    "    \n",
    "    # Calculate overall missing percentage\n",
    "    total_cells = merged_df.shape[0] * merged_df.shape[1]\n",
    "    missing_cells = missing.sum()\n",
    "    missing_pct = (missing_cells / total_cells) * 100\n",
    "    \n",
    "    print(f\"  Overall missing data: {missing_pct:.1f}%\")\n",
    "    \n",
    "    # Show columns with >20% missing\n",
    "    high_missing = []\n",
    "    for col, count in missing_cols.items():\n",
    "        pct = count / len(merged_df) * 100\n",
    "        if pct > 20:\n",
    "            high_missing.append((col, count, pct))\n",
    "    \n",
    "    if high_missing:\n",
    "        print(f\"\\n  Columns with >20% missing values:\")\n",
    "        for col, count, pct in sorted(high_missing, key=lambda x: x[2], reverse=True)[:10]:\n",
    "            print(f\"    {col:35} {count:3} missing ({pct:5.1f}%)\")\n",
    "else:\n",
    "    print(\"✓ No missing values!\")\n",
    "\n",
    "# 4. Check column naming consistency\n",
    "print(f\"\\nColumn naming patterns:\")\n",
    "prefixes = {}\n",
    "for col in merged_df.columns:\n",
    "    if '_' in col:\n",
    "        prefix = col.split('_')[0]\n",
    "        prefixes[prefix] = prefixes.get(prefix, 0) + 1\n",
    "\n",
    "print(f\"  Unique prefixes: {len(prefixes)}\")\n",
    "print(f\"  Most common prefixes:\")\n",
    "for prefix, count in sorted(prefixes.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f\"    {prefix:10} : {count:3} columns\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: Save the cleaned data\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING CLEANED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save merged dataset\n",
    "output_path = PROCESSED_PATH / 'nepal_districts_cleaned.csv'\n",
    "merged_df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "print(f\"✓ Cleaned data saved to: {output_path}\")\n",
    "\n",
    "# Save a simplified version\n",
    "district_list = merged_df[['ID', 'DISTRICT_NAME']].copy()\n",
    "district_list.to_csv(PROCESSED_PATH / 'district_list.csv', index=False)\n",
    "print(f\"✓ District list saved\")\n",
    "\n",
    "# Create and save comprehensive summary\n",
    "summary_path = PROCESSED_PATH / 'cleaning_summary.txt'\n",
    "with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"=\"*70 + \"\\n\")\n",
    "    f.write(\"NEPAL DISTRICT DATA CLEANING - COMPREHENSIVE SUMMARY\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Data Shape: {merged_df.shape[0]} districts × {merged_df.shape[1]} features\\n\")\n",
    "    f.write(f\"Total Files Processed: {len(district_dfs)}\\n\")\n",
    "    f.write(f\"Files with ID columns: {len(files_with_ids)}\\n\")\n",
    "    f.write(f\"Files without ID columns: {len(files_without_ids)}\\n\\n\")\n",
    "    \n",
    "    f.write(\"-\"*70 + \"\\n\")\n",
    "    f.write(\"DATA QUALITY METRICS\\n\")\n",
    "    f.write(\"-\"*70 + \"\\n\")\n",
    "    f.write(f\"Missing values: {missing_cells if 'missing_cells' in locals() else 0} cells\\n\")\n",
    "    if 'missing_pct' in locals():\n",
    "        f.write(f\"Missing percentage: {missing_pct:.1f}%\\n\")\n",
    "    f.write(f\"Duplicate districts: {len(duplicate_ids)}\\n\")\n",
    "    f.write(f\"Expected districts: 77, Found: {len(merged_df)}\\n\\n\")\n",
    "    \n",
    "    f.write(\"-\"*70 + \"\\n\")\n",
    "    f.write(\"COLUMN CATEGORIES\\n\")\n",
    "    f.write(\"-\"*70 + \"\\n\")\n",
    "    \n",
    "    # Categorize columns\n",
    "    categories = {\n",
    "        'Housing': lambda x: any(kw in x for kw in ['WALL', 'ROOF', 'FLOOR', 'FOUNDATION', 'HOUSE']),\n",
    "        'Water/Sanitation': lambda x: any(kw in x for kw in ['WATER', 'TOILET', 'BATH', 'DRI_', 'TOI_']),\n",
    "        'Energy': lambda x: any(kw in x for kw in ['FUEL', 'COOKING', 'LIGHT', 'ELECTRICITY', 'ENERGY']),\n",
    "        'Amenities': lambda x: any(kw in x for kw in ['AMENITY', 'RADIO', 'TV', 'PHONE', 'INTERNET', 'COMPUTER']),\n",
    "        'Demographics': lambda x: any(kw in x for kw in ['CHILDREN', 'POPULATION', 'AGE', 'SEX', 'MARITAL']),\n",
    "        'Education': lambda x: any(kw in x for kw in ['EDUCATION', 'SCHOOL', 'LITERACY', 'DEGREE', 'ATTAINMENT']),\n",
    "        'Employment': lambda x: any(kw in x for kw in ['WORK', 'OCCUPATION', 'EMPLOYMENT', 'INDUSTRY', 'JOB']),\n",
    "        'Metadata': lambda x: x in ['ID', 'DISTRICT_NAME', 'AREA_TYPE', 'AREA_LEVEL']\n",
    "    }\n",
    "    \n",
    "    categorized = {cat: [] for cat in categories.keys()}\n",
    "    uncategorized = []\n",
    "    \n",
    "    for col in merged_df.columns:\n",
    "        categorized_flag = False\n",
    "        for cat, test_func in categories.items():\n",
    "            if test_func(col.upper()):\n",
    "                categorized[cat].append(col)\n",
    "                categorized_flag = True\n",
    "                break\n",
    "        if not categorized_flag:\n",
    "            uncategorized.append(col)\n",
    "    \n",
    "    for cat, cols in categorized.items():\n",
    "        if cols:\n",
    "            f.write(f\"\\n{cat} ({len(cols)} features):\\n\")\n",
    "            for col in sorted(cols)[:15]:  # Show first 15\n",
    "                f.write(f\"  - {col}\\n\")\n",
    "            if len(cols) > 15:\n",
    "                f.write(f\"    ... and {len(cols)-15} more\\n\")\n",
    "    \n",
    "    if uncategorized:\n",
    "        f.write(f\"\\nUncategorized ({len(uncategorized)} features):\\n\")\n",
    "        for col in sorted(uncategorized)[:20]:\n",
    "            f.write(f\"  - {col}\\n\")\n",
    "        if len(uncategorized) > 20:\n",
    "            f.write(f\"    ... and {len(uncategorized)-20} more\\n\")\n",
    "    \n",
    "    f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "    f.write(\"DISTRICT LIST\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    for _, row in merged_df[['ID', 'DISTRICT_NAME']].iterrows():\n",
    "        f.write(f\"{row['ID']:3} : {row['DISTRICT_NAME']}\\n\")\n",
    "\n",
    "print(f\"✓ Summary saved to: {summary_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SCRIPT COMPLETE - READY FOR FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNEXT STEP:\")\n",
    "print(\"1. Open 03_feature_engineering.py\")\n",
    "print(\"2. Key tasks:\")\n",
    "print(\"   - Convert counts to percentages\")\n",
    "print(\"   - Create vulnerability indices\")\n",
    "print(\"   - Handle any remaining missing data\")\n",
    "print(f\"\\nYour cleaned data is ready at: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
